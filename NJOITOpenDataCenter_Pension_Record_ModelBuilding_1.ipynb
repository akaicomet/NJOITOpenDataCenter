{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NJOITOpenDataCenter-Pension-Record-ModelBuilding-1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akaicomet/NJOITOpenDataCenter-Pension-Record/blob/master/NJOITOpenDataCenter_Pension_Record_ModelBuilding_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "L2A4r8Emy1vE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install tqdm\n",
        "!pip install scikit-learn\n",
        "!pip install sodapy\n",
        "!pip install matplotlib\n",
        "!pip install hyperopt\n",
        "!pip install bayesian-optimization\n",
        "!pip install tbvaccine \n",
        "!pip install MulticoreTSNE\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JPRrqTJr11-4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#1. Copy python files from  GItHub repository to local"
      ]
    },
    {
      "metadata": {
        "id": "4yirMDyHy6HF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/akaicomet/NJOITOpenDataCenter.git\n",
        "!git clone https://github.com/akaicomet/akaicomet_Utility.git\n",
        "!git clone https://akaicomet:qwerty123SEP@github.com/akaicomet/akaicomet_Chart.git\n",
        "!git clone https://akaicomet:qwerty123SEP@github.com/akaicomet/akaicomet_ML.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R80YSeU836YS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2. Run .py files in local"
      ]
    },
    {
      "metadata": {
        "id": "dl8bTQ7Ms7hJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%run -i /content/akaicomet_Utility/ErrorHandling.py\n",
        "\n",
        "%run -i /content/NJOITOpenDataCenter/YourMoneyActivePensionMembers.py \n",
        "%run -i /content/NJOITOpenDataCenter/YourMoneyRetiredPensionMembers.py\n",
        "%run -i /content/NJOITOpenDataCenter/YourMoneyPensionMembersMerge.py\n",
        "\n",
        "%run -i /content/akaicomet_Chart/X-Ray_Scan.py\n",
        "%run -i /content/akaicomet_Chart/akaicomet_Chart.py\n",
        "\n",
        "%run -i /content/akaicomet_ML/ML_Init.py\n",
        "%run -i /content/akaicomet_ML/ML_Auto.py\n",
        "%run -i /content/akaicomet_ML/AutoFeatureEngineering.py\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ML9MALdY4Z74",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#3. Import libraries"
      ]
    },
    {
      "metadata": {
        "id": "yAmvqv05tHPz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sodapy import Socrata\n",
        "#from pandas.plotting import scatter_matrix\n",
        "#from datetime import datetime\n",
        "import seaborn as sb\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from textwrap import wrap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "905gCAXlsnlH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.externals import joblib\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bshpCLEA8PcF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#4. YourMoney and the NJOIT Open Data Center \n",
        "##4.1 YourMoney Active Pension Members API\n",
        "\n",
        "**Pension member data load from NJOIT** "
      ]
    },
    {
      "metadata": {
        "id": "jEJfm4bqtKfJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "client = Socrata(\"data.nj.gov\", None)\n",
        "results = client.get(\"45bd-gwii\", limit=100000000)#100000000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "plM06Z5vBlAh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Cleansing and Preparation for YourMoney Active Pension Members data**"
      ]
    },
    {
      "metadata": {
        "id": "_X0n-m4XtWdT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ActivePension_df = ActivePensionDataWrangler(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AvLEEgGb9pbg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##4.2 YourMoney Retired Pension Members API\n",
        "**Pension payment member data load from NJOIT**"
      ]
    },
    {
      "metadata": {
        "id": "Pmixfdlx3a5H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results = client.get(\"8up4-62p6\",   limit=100000000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ysBme-6pAl4G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Cleansing and Preparation for YourMoney Retired Pension data**"
      ]
    },
    {
      "metadata": {
        "id": "VMnmExggP835",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RetiredPension_df_grped = RetiredPensionMembersDataWrangler(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2RMSdT2fCMvo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Merge YourMoney Active Pension Members data and YourMoney Retired Pension data**"
      ]
    },
    {
      "metadata": {
        "id": "gDDk7d-CV8jm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ActivePension_df = PensionMembersMerge(ActivePension_df,RetiredPension_df_grped)\n",
        "del results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QKY-9NBXlCSQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5 Recheck  Correlation Variable Validation:"
      ]
    },
    {
      "metadata": {
        "id": "iK84o24evCgM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#x_cols2 = ['_20_year_status',\t'all_employers_salary_amt',\t\t'employer_freq_pensioned', 'total_months_qty',\t'location_freq_pensioned',\t\t'current_employer_salary_rollingamt', \t'service_months_qty',\t'pension_freq_pensioned', 'pension_fund_name_enc', 'pension_group_name_enc' ]\n",
        "x_cols2 = ['_20_year_status','all_employers_salary_amt','employer_freq_pensioned','total_months_qty','location_freq_pensioned','service_months_qty','pension_freq_pensioned','pension_fund_name_enc','pension_group_name_enc' ]\n",
        "ActivePension_df[x_cols2].corr()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n6MskxRAnJnm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6. Model Selection and Building\n",
        "## 6.1.1 Run Several ML models with hyper-parameter selection and scaling & dimensioning data\n",
        "\n",
        "Data are passed to create new data set (raw data, scaling data, scaling and dimensioning data). Apply dimensioning algorithms to each data and return cross validation results. 180 sec rule is enforced so that algorithm calculation taking 180 sec or mor is not well converged to a certain value and should be discarded.  \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zY8MDlRGu3N8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train = ActivePension_df.sample(n=5000, random_state=0)['IsPensionPaid'].as_matrix()\n",
        "X_train = ActivePension_df[x_cols2].sample(n=5000, random_state=0)\n",
        "\n",
        "#y_train = ActivePension_df['IsPensionPaid']\n",
        "#X_train = ActivePension_df[x_cols2]\n",
        "\n",
        "X_train = X_train.astype(float)\n",
        "#X_train = np.round(X_train, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d5DewDzq2fbr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "manager = Manager()\n",
        "clfs = manager.dict()\n",
        "clfs_score = manager.dict()\n",
        "\n",
        "scaling_def = {'MinMaxScaler': MinMaxScaler(),'Normalizer': Normalizer(),'StandardScaler': StandardScaler(),'PowerTransformer':PowerTransformer(),'QuantileTransformer':QuantileTransformer()}\n",
        "redemensioning_def = {'Raw':None, 'PCA': PCA(n_components=0.99, svd_solver='full'),'MDS': MDS(n_components=len(X_train.columns),n_jobs=-1),'ICA': FastICA(n_components=len(X_train.columns)),'KPCA': KernelPCA(n_components=len(X_train.columns), kernel='rbf',n_jobs=-1)}\n",
        "#redemensioning_def = {'Raw':None, 'PCA': PCA(n_components=0.99, svd_solver='full'),'MDS': MDS(n_components=len(X_train.columns),n_jobs=-1),'ICA': FastICA(n_components=len(X_train.columns)),'KPCA': KernelPCA(n_components=len(X_train.columns), kernel='rbf',n_jobs=-1), 'MulticoreTSNE':MulticoreTSNE(n_components=len(X_train.columns),n_jobs=multiprocessing.cpu_count())}\n",
        "\n",
        "#for data in map(lambda x:Redimensioning2(x[1],x[0],redemensioning_def), scaling(X_train,scaling_def)):\n",
        "for data in map(lambda x:Redimensioning2(x[1],x[0],redemensioning_def), scaling(X_train,scaling_def)):\n",
        "  for x in data:\n",
        "    p1 = Process(target=MLModelBuilding, args=('SVC', x[1], x[0], y_train, 0.3, clfs,[0.9,0.1],clfs_score))\n",
        "    p2 = Process(target=MLModelBuilding, args=('RandomForestClassifier', x[1], x[0], y_train, 0.3, clfs, [0.9,0.1], clfs_score))\n",
        "    p3 = Process(target=MLModelBuilding, args=('LinearSVC', x[1], x[0], y_train, 0.3, clfs, [0.9,0.1], clfs_score))\n",
        "    p4 = Process(target=MLModelBuilding, args=('GradientBoostingClassifier', x[1], x[0], y_train, 0.3, clfs, [0.9,0.1], clfs_score))\n",
        "    p5 = Process(target=MLModelBuilding, args=('AdaBoostClassifier', x[1], x[0], y_train, 0.3, clfs, [0.9,0.1], clfs_score))\n",
        "    p6 = Process(target=MLModelBuilding, args=('LogisticRegression2', x[1], x[0], y_train, 0.3, clfs, [0.9,0.1], clfs_score))\n",
        "    \n",
        "    p1.start()\n",
        "    p2.start()\n",
        "    p3.start()\n",
        "    p4.start()\n",
        "    p5.start()    \n",
        "    p6.start()\n",
        "\n",
        "    p1.join(180)\n",
        "    p2.join(180)\n",
        "    p3.join(180)\n",
        "    p4.join(180)\n",
        "    p5.join(180)\n",
        "    p6.join(180)\n",
        "    \n",
        "    p1.terminate()\n",
        "    p2.terminate()\n",
        "    p3.terminate()\n",
        "    p4.terminate()\n",
        "    p5.terminate()\n",
        "    p6.terminate()\n",
        "    \n",
        "    \n",
        "#manager.shutdown()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1b6nd4H0pqi2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.1.2 Compress and Store Clasification Clasifier to Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "a_9LCwOepjab",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clfs2 = clfs.copy()\n",
        "clfs_score2 = clfs_score.copy()\n",
        "\n",
        "joblib.dump(clfs2, 'clfs612.pkl', compress=True)\n",
        "joblib.dump(clfs_score2, 'clfs_score612.pkl', compress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q1CmlcL5pmPM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "folder_id = '1wRbAVDSYm-NJsRMUqkRsreELnGaNM88R'\n",
        "#file = drive.CreateFile({'clfs':'clfs612.pkl', 'mimeType': 'content/clfs612.pkl','parents': [{'kind': 'drive#fileLink', 'id':folder_id}]})\n",
        "file = drive.CreateFile({'mimeType': 'content/clfs612.pkl','parents': [{'kind': 'drive#fileLink', 'id':folder_id}]})\n",
        "file.SetContentFile('clfs612.pkl')\n",
        "file.Upload() \n",
        "file = drive.CreateFile({'mimeType': 'content/clfs_score612.pkl','parents': [{'kind': 'drive#fileLink', 'id':folder_id}]})\n",
        "file.SetContentFile('clfs_score612.pkl')\n",
        "file.Upload() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DF1E76wA7gMG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.2 Further Refinement: MinMaxScaler & QuantileTransformer for GradientBoostingClassifier"
      ]
    },
    {
      "metadata": {
        "id": "kce_EYx9AAf9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.2.3 Resampling to Validate Model "
      ]
    },
    {
      "metadata": {
        "id": "dQwL45Ht7YTW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clfs_score2 = dict()\n",
        "\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "y_train = ActivePension_df.sample(n=1000, random_state=1)['IsPensionPaid'].as_matrix()\n",
        "X_train = ActivePension_df[x_cols2].sample(n=1000, random_state=1)\n",
        "\n",
        "#y_train = ActivePension_df['IsPensionPaid'].as_matrix()\n",
        "#X_train = ActivePension_df[x_cols2]\n",
        "\n",
        "X_train = X_train.astype(float)\n",
        "\n",
        "for clf in clfs:\n",
        "  if clfs_score[clf][0] > 0.8:\n",
        "    scaling_def_temp = {i: scaling_def[i] for i in flatten([i.split(' ') for i in clf.split('|')])[1:] if i in scaling_def}\n",
        "    redemensioning_def_temp = {i: redemensioning_def[i] for i in flatten([i.split(' ') for i in clf.split('|')])[1:] if i in redemensioning_def}\n",
        "    for data in map(lambda x:Redimensioning2(x[1],x[0],redemensioning_def_temp), scaling3(X_train,scaling_def_temp)):\n",
        "      for x in data:\n",
        "        score = clfs[clf].score(x[0], y_train)\n",
        "        if score > 0.8:\n",
        "          print(x[1])\n",
        "          print('Train score: {}'.format(clfs_score[clf][0]))\n",
        "          print('Test score: {}'.format(score))\n",
        "          clfs_score2[clf] = [score]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A0evzvR8BJ4K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clfs_score22 = clfs_score2.copy()\n",
        "\n",
        "joblib.dump(clfs_score22, 'clfs_score623.pkl', compress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ehTYWASvBKEH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "folder_id = '1wRbAVDSYm-NJsRMUqkRsreELnGaNM88R'\n",
        "file = drive.CreateFile({'clfs':'clfs_score623.pkl', 'mimeType': 'content/clfs_score623.pkl','parents': [{'kind': 'drive#fileLink', 'id':folder_id}]})\n",
        "file.SetContentFile('clfs_score623.pkl')\n",
        "file.Upload() "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}